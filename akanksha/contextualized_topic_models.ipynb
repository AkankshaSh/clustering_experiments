{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextualized_topic_models\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.utils.data_preparation import TextHandler\n",
    "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "\n",
    "# tqdm allows you to create progress bars to track how long your code is taking to process\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "# pprint is to make our topics formatted a little nicer when we take a look\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akankshasharma/Library/Python/3.7/lib/python/site-packages/contextualized_topic_models/utils/data_preparation.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.vocab_dict[x], y.split()))), data)))\n"
     ]
    }
   ],
   "source": [
    "handler = TextHandler(\"documents.txt\")\n",
    "handler.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100, 100)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/100]\tSamples: [35/3500]\tTrain Loss: 212.7273158482143\tTime: 0:00:00.163821\n",
      "Epoch: [2/100]\tSamples: [70/3500]\tTrain Loss: 214.52566964285714\tTime: 0:00:00.078270\n",
      "Epoch: [3/100]\tSamples: [105/3500]\tTrain Loss: 211.35712890625\tTime: 0:00:00.133631\n",
      "Epoch: [4/100]\tSamples: [140/3500]\tTrain Loss: 209.90057198660713\tTime: 0:00:00.075811\n",
      "Epoch: [5/100]\tSamples: [175/3500]\tTrain Loss: 210.4786411830357\tTime: 0:00:00.094209\n",
      "Epoch: [6/100]\tSamples: [210/3500]\tTrain Loss: 211.3962890625\tTime: 0:00:00.071848\n",
      "Epoch: [7/100]\tSamples: [245/3500]\tTrain Loss: 211.37709263392858\tTime: 0:00:00.073779\n",
      "Epoch: [8/100]\tSamples: [280/3500]\tTrain Loss: 211.00973772321427\tTime: 0:00:00.080701\n",
      "Epoch: [9/100]\tSamples: [315/3500]\tTrain Loss: 210.06201171875\tTime: 0:00:00.077230\n",
      "Epoch: [10/100]\tSamples: [350/3500]\tTrain Loss: 210.19080636160714\tTime: 0:00:00.081571\n",
      "Epoch: [11/100]\tSamples: [385/3500]\tTrain Loss: 213.08219866071428\tTime: 0:00:00.077921\n",
      "Epoch: [12/100]\tSamples: [420/3500]\tTrain Loss: 211.6552734375\tTime: 0:00:00.069349\n",
      "Epoch: [13/100]\tSamples: [455/3500]\tTrain Loss: 209.88777901785716\tTime: 0:00:00.074026\n",
      "Epoch: [14/100]\tSamples: [490/3500]\tTrain Loss: 211.697265625\tTime: 0:00:00.124137\n",
      "Epoch: [15/100]\tSamples: [525/3500]\tTrain Loss: 209.8141880580357\tTime: 0:00:00.080020\n",
      "Epoch: [16/100]\tSamples: [560/3500]\tTrain Loss: 209.4071010044643\tTime: 0:00:00.077112\n",
      "Epoch: [17/100]\tSamples: [595/3500]\tTrain Loss: 211.97059151785714\tTime: 0:00:00.079244\n",
      "Epoch: [18/100]\tSamples: [630/3500]\tTrain Loss: 209.70160435267857\tTime: 0:00:00.078728\n",
      "Epoch: [19/100]\tSamples: [665/3500]\tTrain Loss: 209.87840401785715\tTime: 0:00:00.080572\n",
      "Epoch: [20/100]\tSamples: [700/3500]\tTrain Loss: 209.69087611607142\tTime: 0:00:00.077180\n",
      "Epoch: [21/100]\tSamples: [735/3500]\tTrain Loss: 208.28653738839284\tTime: 0:00:00.079868\n",
      "Epoch: [22/100]\tSamples: [770/3500]\tTrain Loss: 208.55343191964286\tTime: 0:00:00.077299\n",
      "Epoch: [23/100]\tSamples: [805/3500]\tTrain Loss: 208.90852399553572\tTime: 0:00:00.075270\n",
      "Epoch: [24/100]\tSamples: [840/3500]\tTrain Loss: 209.18589564732142\tTime: 0:00:00.078384\n",
      "Epoch: [25/100]\tSamples: [875/3500]\tTrain Loss: 207.43853236607143\tTime: 0:00:00.081183\n",
      "Epoch: [26/100]\tSamples: [910/3500]\tTrain Loss: 211.54977678571427\tTime: 0:00:00.077813\n",
      "Epoch: [27/100]\tSamples: [945/3500]\tTrain Loss: 209.80485491071428\tTime: 0:00:00.089117\n",
      "Epoch: [28/100]\tSamples: [980/3500]\tTrain Loss: 209.39990234375\tTime: 0:00:00.077140\n",
      "Epoch: [29/100]\tSamples: [1015/3500]\tTrain Loss: 209.31296037946427\tTime: 0:00:00.075298\n",
      "Epoch: [30/100]\tSamples: [1050/3500]\tTrain Loss: 207.38212890625\tTime: 0:00:00.079952\n",
      "Epoch: [31/100]\tSamples: [1085/3500]\tTrain Loss: 206.55689174107144\tTime: 0:00:00.076140\n",
      "Epoch: [32/100]\tSamples: [1120/3500]\tTrain Loss: 206.82113560267857\tTime: 0:00:00.079131\n",
      "Epoch: [33/100]\tSamples: [1155/3500]\tTrain Loss: 208.20186941964286\tTime: 0:00:00.079803\n",
      "Epoch: [34/100]\tSamples: [1190/3500]\tTrain Loss: 208.1265904017857\tTime: 0:00:00.075134\n",
      "Epoch: [35/100]\tSamples: [1225/3500]\tTrain Loss: 204.50297154017858\tTime: 0:00:00.075272\n",
      "Epoch: [36/100]\tSamples: [1260/3500]\tTrain Loss: 205.1037806919643\tTime: 0:00:00.084517\n",
      "Epoch: [37/100]\tSamples: [1295/3500]\tTrain Loss: 207.2145786830357\tTime: 0:00:00.079678\n",
      "Epoch: [38/100]\tSamples: [1330/3500]\tTrain Loss: 205.40414341517857\tTime: 0:00:00.077138\n",
      "Epoch: [39/100]\tSamples: [1365/3500]\tTrain Loss: 208.20594308035714\tTime: 0:00:00.109875\n",
      "Epoch: [40/100]\tSamples: [1400/3500]\tTrain Loss: 206.46417410714287\tTime: 0:00:00.101959\n",
      "Epoch: [41/100]\tSamples: [1435/3500]\tTrain Loss: 205.9508091517857\tTime: 0:00:00.081808\n",
      "Epoch: [42/100]\tSamples: [1470/3500]\tTrain Loss: 203.01097935267856\tTime: 0:00:00.078706\n",
      "Epoch: [43/100]\tSamples: [1505/3500]\tTrain Loss: 204.51533203125\tTime: 0:00:00.078645\n",
      "Epoch: [44/100]\tSamples: [1540/3500]\tTrain Loss: 204.39744698660715\tTime: 0:00:00.084207\n",
      "Epoch: [45/100]\tSamples: [1575/3500]\tTrain Loss: 202.92544642857143\tTime: 0:00:00.083114\n",
      "Epoch: [46/100]\tSamples: [1610/3500]\tTrain Loss: 202.81396484375\tTime: 0:00:00.079731\n",
      "Epoch: [47/100]\tSamples: [1645/3500]\tTrain Loss: 203.01014229910714\tTime: 0:00:00.081197\n",
      "Epoch: [48/100]\tSamples: [1680/3500]\tTrain Loss: 202.74015066964284\tTime: 0:00:00.086325\n",
      "Epoch: [49/100]\tSamples: [1715/3500]\tTrain Loss: 201.0481166294643\tTime: 0:00:00.090914\n",
      "Epoch: [50/100]\tSamples: [1750/3500]\tTrain Loss: 202.95669642857143\tTime: 0:00:00.128564\n",
      "Epoch: [51/100]\tSamples: [1785/3500]\tTrain Loss: 203.87307477678573\tTime: 0:00:00.139422\n",
      "Epoch: [52/100]\tSamples: [1820/3500]\tTrain Loss: 203.78920200892858\tTime: 0:00:00.105774\n",
      "Epoch: [53/100]\tSamples: [1855/3500]\tTrain Loss: 201.08897879464286\tTime: 0:00:00.089019\n",
      "Epoch: [54/100]\tSamples: [1890/3500]\tTrain Loss: 204.31739676339285\tTime: 0:00:00.080509\n",
      "Epoch: [55/100]\tSamples: [1925/3500]\tTrain Loss: 201.92917131696427\tTime: 0:00:00.075971\n",
      "Epoch: [56/100]\tSamples: [1960/3500]\tTrain Loss: 202.08720703125\tTime: 0:00:00.076656\n",
      "Epoch: [57/100]\tSamples: [1995/3500]\tTrain Loss: 200.4775390625\tTime: 0:00:00.078988\n",
      "Epoch: [58/100]\tSamples: [2030/3500]\tTrain Loss: 199.19270368303572\tTime: 0:00:00.078461\n",
      "Epoch: [59/100]\tSamples: [2065/3500]\tTrain Loss: 201.34886997767856\tTime: 0:00:00.076241\n",
      "Epoch: [60/100]\tSamples: [2100/3500]\tTrain Loss: 201.46107700892858\tTime: 0:00:00.081635\n",
      "Epoch: [61/100]\tSamples: [2135/3500]\tTrain Loss: 196.94790736607143\tTime: 0:00:00.091085\n",
      "Epoch: [62/100]\tSamples: [2170/3500]\tTrain Loss: 200.04446149553573\tTime: 0:00:00.083412\n",
      "Epoch: [63/100]\tSamples: [2205/3500]\tTrain Loss: 199.78491908482144\tTime: 0:00:00.080407\n",
      "Epoch: [64/100]\tSamples: [2240/3500]\tTrain Loss: 200.47013113839284\tTime: 0:00:00.083224\n",
      "Epoch: [65/100]\tSamples: [2275/3500]\tTrain Loss: 198.8337611607143\tTime: 0:00:00.088949\n",
      "Epoch: [66/100]\tSamples: [2310/3500]\tTrain Loss: 197.72356305803572\tTime: 0:00:00.084976\n",
      "Epoch: [67/100]\tSamples: [2345/3500]\tTrain Loss: 195.66128627232143\tTime: 0:00:00.128750\n",
      "Epoch: [68/100]\tSamples: [2380/3500]\tTrain Loss: 195.59700055803572\tTime: 0:00:00.077072\n",
      "Epoch: [69/100]\tSamples: [2415/3500]\tTrain Loss: 197.066015625\tTime: 0:00:00.085270\n",
      "Epoch: [70/100]\tSamples: [2450/3500]\tTrain Loss: 197.89161551339285\tTime: 0:00:00.076122\n",
      "Epoch: [71/100]\tSamples: [2485/3500]\tTrain Loss: 194.91646205357142\tTime: 0:00:00.097706\n",
      "Epoch: [72/100]\tSamples: [2520/3500]\tTrain Loss: 197.42958984375\tTime: 0:00:00.124421\n",
      "Epoch: [73/100]\tSamples: [2555/3500]\tTrain Loss: 194.83232421875\tTime: 0:00:00.079975\n",
      "Epoch: [74/100]\tSamples: [2590/3500]\tTrain Loss: 196.11704799107142\tTime: 0:00:00.077507\n",
      "Epoch: [75/100]\tSamples: [2625/3500]\tTrain Loss: 196.8781529017857\tTime: 0:00:00.078563\n",
      "Epoch: [76/100]\tSamples: [2660/3500]\tTrain Loss: 195.24946986607142\tTime: 0:00:00.080182\n",
      "Epoch: [77/100]\tSamples: [2695/3500]\tTrain Loss: 196.4427734375\tTime: 0:00:00.079902\n",
      "Epoch: [78/100]\tSamples: [2730/3500]\tTrain Loss: 195.34525669642858\tTime: 0:00:00.082084\n",
      "Epoch: [79/100]\tSamples: [2765/3500]\tTrain Loss: 193.31633649553572\tTime: 0:00:00.077407\n",
      "Epoch: [80/100]\tSamples: [2800/3500]\tTrain Loss: 194.52546037946428\tTime: 0:00:00.076796\n",
      "Epoch: [81/100]\tSamples: [2835/3500]\tTrain Loss: 192.6099888392857\tTime: 0:00:00.109296\n",
      "Epoch: [82/100]\tSamples: [2870/3500]\tTrain Loss: 195.41508091517858\tTime: 0:00:00.092871\n",
      "Epoch: [83/100]\tSamples: [2905/3500]\tTrain Loss: 190.78412388392857\tTime: 0:00:00.077609\n",
      "Epoch: [84/100]\tSamples: [2940/3500]\tTrain Loss: 193.80219029017857\tTime: 0:00:00.100498\n",
      "Epoch: [85/100]\tSamples: [2975/3500]\tTrain Loss: 194.73497488839286\tTime: 0:00:00.085360\n",
      "Epoch: [86/100]\tSamples: [3010/3500]\tTrain Loss: 192.70380859375\tTime: 0:00:00.080010\n",
      "Epoch: [87/100]\tSamples: [3045/3500]\tTrain Loss: 190.83578404017857\tTime: 0:00:00.079297\n",
      "Epoch: [88/100]\tSamples: [3080/3500]\tTrain Loss: 189.1624720982143\tTime: 0:00:00.082751\n",
      "Epoch: [89/100]\tSamples: [3115/3500]\tTrain Loss: 192.5089146205357\tTime: 0:00:00.077586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [90/100]\tSamples: [3150/3500]\tTrain Loss: 190.3243443080357\tTime: 0:00:00.086061\n",
      "Epoch: [91/100]\tSamples: [3185/3500]\tTrain Loss: 190.6351283482143\tTime: 0:00:00.082367\n",
      "Epoch: [92/100]\tSamples: [3220/3500]\tTrain Loss: 190.91953125\tTime: 0:00:00.087674\n",
      "Epoch: [93/100]\tSamples: [3255/3500]\tTrain Loss: 187.64323381696428\tTime: 0:00:00.085840\n",
      "Epoch: [94/100]\tSamples: [3290/3500]\tTrain Loss: 189.34839564732144\tTime: 0:00:00.078068\n",
      "Epoch: [95/100]\tSamples: [3325/3500]\tTrain Loss: 191.9134486607143\tTime: 0:00:00.076335\n",
      "Epoch: [96/100]\tSamples: [3360/3500]\tTrain Loss: 187.87861328125\tTime: 0:00:00.087560\n",
      "Epoch: [97/100]\tSamples: [3395/3500]\tTrain Loss: 186.5501953125\tTime: 0:00:00.073873\n",
      "Epoch: [98/100]\tSamples: [3430/3500]\tTrain Loss: 188.78759765625\tTime: 0:00:00.069172\n",
      "Epoch: [99/100]\tSamples: [3465/3500]\tTrain Loss: 190.42066127232144\tTime: 0:00:00.071778\n",
      "Epoch: [100/100]\tSamples: [3500/3500]\tTrain Loss: 188.47459542410715\tTime: 0:00:00.082685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate BERT data\n",
    "training_bert = bert_embeddings_from_file(\"documents.txt\", \"distiluse-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "training_dataset = CTMDataset(handler.bow, training_bert, handler.idx2token)\n",
    "\n",
    "ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, inference_type=\"combined\", n_components=50)\n",
    "\n",
    "ctm.fit(training_dataset) # run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get topic list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['fundraiser', 'in', 'relatively', 'aiming', 'will'],\n",
       " ['rate', 'The', 'To', 'contract', 'keep'],\n",
       " ['couriers', 'televised', 'and', 'rules', 'Already,'],\n",
       " ['stepped', 'coronavirus,', 'million', 'far', 'Some'],\n",
       " ['under', 'the', 'last', 'economy', 'women'],\n",
       " ['New', 'laboratory-confirmed', 'tube,', 'arriving', 'travel.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Get topic list\")\n",
    "ctm.get_topic_lists(5)[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The World Health Organization reported a record increase in global coronavirus infections yesterday, with the total rising by 183,020 in a 24-hour period.\r\n",
      "\r\n",
      "The biggest increase, of more than 116,000, was from North and South America, it said in a daily report. Total global cases have passed 8.7 million with more than 461,000 deaths, the WHO added.\r\n",
      "\r\n",
      "Germany's coronavirus reproduction rate jumped to 2.88 yesterday, up from 1.79 a day earlier, health authorities said, a rate showing infections are rising above the level needed to contain the disease over the longer term.\r\n",
      "\r\n",
      "The rise brings with it the possibility of renewed restrictions on activity in Europe's largest economy - a blow to a country that so far had widely been seen as successful in curbing the coronavirus spread and keeping the death toll relatively low.\r\n",
      "\r\n",
      "To keep the pandemic under control, Germany needs the reproduction rate to drop below one. The rate of 2.88, published by the Robert Koch Institute (RKI) for public health, means that out of 100 people who contract the virus, a further 288 people will get infected.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 documents.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.003234884236007929, 0.07325280457735062, 0.007807305548340082, 0.24313412606716156, 0.006643406115472317, 0.002581328619271517, 0.004334053490310907, 0.0006474740221165121, 0.00915495865046978, 0.016229234635829926, 0.004887876100838184, 0.010647004470229149, 0.0018588844686746597, 0.030883969739079475, 0.003372527426108718, 0.03695008158683777, 0.0032030942384153605, 0.0039533269591629505, 0.039643656462430954, 0.002057476667687297, 0.0029870006255805492, 0.005823671817779541, 0.0062237088568508625, 0.0017570609925314784, 0.012800307944417, 0.005516372621059418, 0.004783101845532656, 0.02290784753859043, 0.02422214485704899, 0.008392039686441422, 0.0024587088264524937, 0.017790278419852257, 0.014246385544538498, 0.008215066976845264, 0.09104691445827484, 0.039396271109580994, 0.038242027163505554, 0.00047126191202551126, 0.006299561820924282, 0.0029759102035313845, 0.022552059963345528, 0.005243094637989998, 0.0348799005150795, 0.001906018704175949, 0.03045138530433178, 0.03727664425969124, 0.007154008373618126, 0.013610586524009705, 0.014144662767648697, 0.011748536489903927]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stepped', 'coronavirus,', 'million', 'far', 'Some']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "distribution = ctm.get_thetas(training_dataset)[8] # topic distribution for the first document\n",
    "\n",
    "print(distribution)\n",
    "\n",
    "topic = np.argmax(distribution)\n",
    "\n",
    "ctm.get_topic_lists(5)[topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Model\n",
    "\n",
    "from contextualized_topic_models.evaluation.measures import TopicDiversity, CoherenceNPMI,\\\n",
    "    CoherenceWordEmbeddings, InvertedRBO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = TopicDiversity(ctm.get_topic_lists(25))\n",
    "td.score(topk=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbo = InvertedRBO(ctm.get_topic_lists(10))\n",
    "rbo.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "td=TopicDiversity(ctm.get_topic_lists(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fundraiser', 'in', 'relatively', 'aiming', 'will'],\n",
       " ['rate', 'The', 'To', 'contract', 'keep'],\n",
       " ['couriers', 'televised', 'and', 'rules', 'Already,'],\n",
       " ['stepped', 'coronavirus,', 'million', 'far', 'Some'],\n",
       " ['under', 'the', 'last', 'economy', 'women'],\n",
       " ['New', 'laboratory-confirmed', 'tube,', 'arriving', 'travel.'],\n",
       " ['to', 'which', 'said', 'world', 'and'],\n",
       " ['reporters.', 'tube,', 'country', 'activity', 'toll'],\n",
       " ['that', 'couriers', 'expected', 'was', 'today,'],\n",
       " ['1.79', 'testing', 'New', 'following', 'tightened'],\n",
       " ['the', 'called', 'allowed', 'around', 'number'],\n",
       " ['the', 'to', 'Allgemeine', 'Bundesbank', 'newspaper'],\n",
       " ['current', 'and', 'Officials', 'news', 'Beijing'],\n",
       " ['and', 'to', 'mitigate', 'linked', 'stepped'],\n",
       " ['in', 'ban,', 'days.', 'but', 'nine.'],\n",
       " ['contract', 'the', 'health,', 'by', 'keep'],\n",
       " ['quarantine', 'As', 'multiple', 'week', 'Some'],\n",
       " ['parcel', 'outbreak,', 'Chinese', 'workers', 'virus'],\n",
       " ['daily', '461,000', 'South', '2.88', 'from'],\n",
       " ['meat', 'companies,', 'religious', \"China's\", 'harvest'],\n",
       " ['have', 'are', 'deaths,', 'South', '116,000,'],\n",
       " ['cruise', 'may', 'China', 'testing', 'facilities.'],\n",
       " ['which', 'a', 'The', 'days.', 'small'],\n",
       " ['expire', 'pressure', '30th', 'the', 'Ardern'],\n",
       " ['infections', 'outbreaks', 'local', 'Rhine-Westphalia,', 'published'],\n",
       " ['but', 'Covid-19,', 'The', 'declaring', 'end'],\n",
       " ['in', 'which', 'hospitals,', 'said.', 'institutions'],\n",
       " ['coronavirus,', 'In', 'was', 'now', 'almost'],\n",
       " ['Chinese', 'has', 'to', 'customs', 'suspended'],\n",
       " ['stepped', 'new', 'cases', 'cluster', 'screening'],\n",
       " ['result,', 'As', 'kindergartens', 'allowed', 'facilities.'],\n",
       " ['ban', 'to', 'country', 'close', 'kindergartens'],\n",
       " ['183,020', 'World', 'small', '24-hour', 'ship'],\n",
       " ['for', 'multiple', 'may', 'daily', 'urging'],\n",
       " ['by', 'Organization', 'a', 'coronavirus', 'World'],\n",
       " ['Guetersloh', 'longer', 'people', 'from', 'needed'],\n",
       " ['due', 'has', 'on', 'running', 'x'],\n",
       " ['further', 'Over', 'both', 'saying', 'means'],\n",
       " ['tightened', 'Jens', 'visitors', 'extended', 'travel.'],\n",
       " ['processor', 'meat', 'overseas', 'occurred', 'products'],\n",
       " ['the', 'passed', 'Weidmann', 'In', 'caused'],\n",
       " ['may', 'test', 'need', 'before', 'negative'],\n",
       " ['a', 'yesterday,', 'said', 'now', 'infections'],\n",
       " ['quarantine', 'police', 'related', 'reboot', 'mainly'],\n",
       " ['coronavirus,', 'tested', 'virtual', 'David', 'border'],\n",
       " ['Music', 'ban,', 'ship', 'evidence', 'host'],\n",
       " ['laboratory-confirmed', 'seekers', 'have', 'infections', 'test'],\n",
       " ['about', 'must', 'Already,', 'regions', 'quarantine'],\n",
       " ['screening', 'by', 'evidence', 'quarantine', 'before'],\n",
       " ['Already,', 'city', 'isolation', 'grappling', '803']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coherence measure based on Word Embeddings\n",
    "#evaluation of coherence on a word embedding space. if word2vec_file is specified, \n",
    "#it retrieves the word embeddings file (in word2vec format) otherwise 'word2vec-google-news-300' is downloaded \n",
    "#using gensim's APIs\n",
    "\n",
    "# td.topics\n",
    "ctm.get_topic_lists(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
